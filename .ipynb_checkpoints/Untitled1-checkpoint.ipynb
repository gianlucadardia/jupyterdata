{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2b5153-36fe-490c-95ae-5a5ffca26a93",
   "metadata": {},
   "source": [
    "# 101 PySpark Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8323d21-65a3-4207-889a-0d03091f16b8",
   "metadata": {},
   "source": [
    "## How to import PySpark and check the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d85a1-e33b-49c2-ae93-41d2618797db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Creating a SparkSession: A SparkSession is the entry point for using the PySpark DataFrame and SQL API.\n",
    "# To create a SparkSession, use the following code\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
    "\n",
    "# Get version details\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497995c-1667-44a8-ba29-b2f32fc333fd",
   "metadata": {},
   "source": [
    "## How to convert the index of a PySpark DataFrame into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9200f5-80d8-4fb4-885e-ca90441f75e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef10173-fd2f-4bb0-ac72-49d5843e6355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acb155-be11-4baa-8ad4-42d83ab8dbd0",
   "metadata": {},
   "source": [
    "## How to get the minimum, 25th percentile, median, 75th, and max of a numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52486889-f5b2-4011-83ef-fac67f2d6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0ffeb-3c0c-4982-9e2e-1ff56e507eae",
   "metadata": {},
   "source": [
    "### https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html\n",
    "\n",
    "pyspark.sql.DataFrame.approxQuantile\n",
    "DataFrame.approxQuantile(col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) â†’ Union[List[float], List[List[float]]][source]\n",
    "Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p * N). More precisely,\n",
    "\n",
    "floor((p - err) * N) <= rank(x) <= ceil((p + err) * N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb6b8bc3-20e8-4fee-bfe3-90553bb36899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentiles\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9df03a8-a70d-4a38-a1b2-7267603d1d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
