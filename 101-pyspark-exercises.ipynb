{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2b5153-36fe-490c-95ae-5a5ffca26a93",
   "metadata": {},
   "source": [
    "# 101 PySpark Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8323d21-65a3-4207-889a-0d03091f16b8",
   "metadata": {},
   "source": [
    "## How to import PySpark and check the version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4d85a1-e33b-49c2-ae93-41d2618797db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Creating a SparkSession: A SparkSession is the entry point for using the PySpark DataFrame and SQL API.\n",
    "# To create a SparkSession, use the following code\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"PySpark 101 Exercises\").getOrCreate()\n",
    "\n",
    "# Get version details\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497995c-1667-44a8-ba29-b2f32fc333fd",
   "metadata": {},
   "source": [
    "## How to convert the index of a PySpark DataFrame into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9200f5-80d8-4fb4-885e-ca90441f75e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  Alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"Alice\", 1),\n",
    "(\"Bob\", 2),\n",
    "(\"Charlie\", 3),\n",
    "], [\"Name\", \"Value\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef10173-fd2f-4bb0-ac72-49d5843e6355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+\n",
      "|   Name|Value|index|\n",
      "+-------+-----+-----+\n",
      "|  Alice|    1|    0|\n",
      "|    Bob|    2|    1|\n",
      "|Charlie|    3|    2|\n",
      "+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "\n",
    "# Define window specification\n",
    "w = Window.orderBy(monotonically_increasing_id())\n",
    "\n",
    "# Add index\n",
    "df = df.withColumn(\"index\", row_number().over(w) - 1)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acb155-be11-4baa-8ad4-42d83ab8dbd0",
   "metadata": {},
   "source": [
    "## How to get the minimum, 25th percentile, median, 75th, and max of a numeric column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52486889-f5b2-4011-83ef-fac67f2d6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|   A| 10|\n",
      "|   B| 20|\n",
      "|   C| 30|\n",
      "|   D| 40|\n",
      "|   E| 50|\n",
      "|   F| 15|\n",
      "|   G| 28|\n",
      "|   H| 54|\n",
      "|   I| 41|\n",
      "|   J| 86|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"D\", 40), (\"E\", 50), (\"F\", 15), (\"G\", 28), (\"H\", 54), (\"I\", 41), (\"J\", 86)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0ffeb-3c0c-4982-9e2e-1ff56e507eae",
   "metadata": {},
   "source": [
    "### https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html\n",
    "\n",
    "**pyspark.sql.DataFrame.approxQuantile**\n",
    "\n",
    "DataFrame.approxQuantile(col: Union[str, List[str], Tuple[str]], probabilities: Union[List[float], Tuple[float]], relativeError: float) â†’ Union[List[float], List[List[float]]][source]\n",
    "\n",
    "Calculates the approximate quantiles of numerical columns of a DataFrame.\n",
    "\n",
    "The result of this algorithm has the following deterministic bound: If the DataFrame has N elements and if we request the quantile at probability p up to error err, then the algorithm will return a sample x from the DataFrame so that the exact rank of x is close to (p * N). More precisely,\n",
    "\n",
    "floor((p - err) * N) <= rank(x) <= ceil((p + err) * N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb6b8bc3-20e8-4fee-bfe3-90553bb36899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min:  10.0\n",
      "25th percentile:  20.0\n",
      "Median:  30.0\n",
      "75th percentile:  50.0\n",
      "Max:  86.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate percentiles\n",
    "quantiles = df.approxQuantile(\"Age\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.01)\n",
    "\n",
    "print(\"Min: \", quantiles[0])\n",
    "print(\"25th percentile: \", quantiles[1])\n",
    "print(\"Median: \", quantiles[2])\n",
    "print(\"75th percentile: \", quantiles[3])\n",
    "print(\"Max: \", quantiles[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670a765-fe06-43c7-93d1-62a9e5b2e57e",
   "metadata": {},
   "source": [
    "## How to get frequency counts of unique items of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1417b3-a5e8-44bf-b8ff-0ac5cd4dc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "spark = SparkSession.builder.appName(\"PySpark Session 02\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ed681b-8ed9-4ff6-b198-2acfa1e79149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|      job|\n",
      "+----+---------+\n",
      "|John| Engineer|\n",
      "|John| Engineer|\n",
      "|Mary|Scientist|\n",
      "| Bob| Engineer|\n",
      "| Bob| Engineer|\n",
      "| Bob|Scientist|\n",
      "| Sam|   Doctor|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='John', job='Engineer'),\n",
    "Row(name='Mary', job='Scientist'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Engineer'),\n",
    "Row(name='Bob', job='Scientist'),\n",
    "Row(name='Sam', job='Doctor'),\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba61ec3-2504-4a37-b5cc-a97838f6ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "| Engineer|    4|\n",
      "|Scientist|    2|\n",
      "|   Doctor|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"job\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e5c8c-5acb-4584-a275-856093b89e09",
   "metadata": {},
   "source": [
    "## How to Drop rows with NA values specific to a particular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3af0ff92-bb48-48c8-88cf-1270be2a58be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B| NULL| 123|\n",
      "|   B|    3| 456|\n",
      "|   D| NULL|NULL|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca03982-daea-486a-a255-3ec870b7b1ea",
   "metadata": {},
   "source": [
    "### https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dropna.html#pyspark.sql.DataFrame.dropna\n",
    "**pyspark.sql.DataFrame.dropna**\n",
    "\n",
    "Returns a new DataFrame omitting rows with null values. DataFrame.dropna() and DataFrameNaFunctions.drop() are aliases of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa95f6e3-ece5-4c4a-91ad-3ae29ea8a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   B|    3|456|\n",
      "+----+-----+---+\n",
      "\n",
      "+----+-----+----+\n",
      "|Name|Value|  id|\n",
      "+----+-----+----+\n",
      "|   A|    1|NULL|\n",
      "|   B|    3| 456|\n",
      "+----+-----+----+\n",
      "\n",
      "+----+-----+---+\n",
      "|Name|Value| id|\n",
      "+----+-----+---+\n",
      "|   B| NULL|123|\n",
      "|   B|    3|456|\n",
      "+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_0 = df.dropna()\n",
    "df_1 = df.dropna(subset=['Value'])\n",
    "df_2 = df.dropna(subset=['id'])\n",
    "\n",
    "df_0.show()\n",
    "df_1.show()\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f1e0f-090a-4c3e-a1e3-0111fd45be99",
   "metadata": {},
   "source": [
    "## How to rename columns of a PySpark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f6a168-b13f-41b2-b608-0f442b438ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|age2| name|\n",
      "+----+-----+\n",
      "|   2|Alice|\n",
      "|   5|  Bob|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "df.withColumnRenamed('age', 'age2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f05d8b-480e-4df4-8c48-f21ea950a8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# suppose you have the following DataFrame\n",
    "df = spark.createDataFrame([(1, 2, 3), (4, 5, 6)], [\"col1\", \"col2\", \"col3\"])\n",
    "# old column names\n",
    "old_names = [\"col1\", \"col2\", \"col3\"]\n",
    "# new column names\n",
    "new_names = [\"new_col1\", \"new_col2\", \"new_col3\"]\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "654236c0-ccc9-451f-ae9a-5bd9fe7eb36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('col1', 'newcol1')\n",
    "df.withColumnRenamed('col2', 'newcol2')\n",
    "df.withColumnRenamed('col3', 'newcol3')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d263acb0-ccd4-4142-8769-7e3023b863ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|newcol1|newcol2|newcol3|\n",
      "+-------+-------+-------+\n",
      "|      1|      2|      3|\n",
      "|      4|      5|      6|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumnRenamed('col1', 'newcol1')\n",
    "df2 = df1.withColumnRenamed('col2', 'newcol2')\n",
    "df3 = df2.withColumnRenamed('col3', 'newcol3')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5033a-fa16-4b4b-8729-11543c16691f",
   "metadata": {},
   "source": [
    "## How to bin a numeric list to 10 groups of equal size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be8fe93-d828-4cdb-a908-e71aadff6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|             values|\n",
      "+-------------------+\n",
      "|  0.619189370225301|\n",
      "| 0.5096018842446481|\n",
      "| 0.8325259388871524|\n",
      "|0.26322809041172357|\n",
      "| 0.6702867696264135|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create a DataFrame with a single column \"values\" filled with random numbers\n",
    "num_items = 100\n",
    "df = spark.range(num_items).select(rand(seed=42).alias(\"values\"))\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f0404ba-52f0-46e6-9327-18c5b0934296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0031434569487398534, 0.13079406894907553, 0.23893974653627081, 0.30357970527906064, 0.4484250584033179, 0.5844552193973006, 0.6702867696264135, 0.7290493814691454, 0.8017532427858894, 0.8505582285081454, 0.9991441647585968]\n"
     ]
    }
   ],
   "source": [
    "# Define the bucket boundaries\n",
    "num_buckets = 10\n",
    "quantiles = df.stat.approxQuantile(\"values\", [i/num_buckets for i in range(num_buckets+1)], 0.01)\n",
    "\n",
    "print (quantiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94770dc2-7205-476c-88c4-a53750bc7607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|             values|buckets|\n",
      "+-------------------+-------+\n",
      "|  0.619189370225301|    5.0|\n",
      "| 0.5096018842446481|    4.0|\n",
      "| 0.8325259388871524|    8.0|\n",
      "|0.26322809041172357|    2.0|\n",
      "| 0.6702867696264135|    6.0|\n",
      "| 0.5173283545794627|    4.0|\n",
      "| 0.9991441647585968|    9.0|\n",
      "|0.06993233728279169|    0.0|\n",
      "| 0.9696695610826327|    9.0|\n",
      "| 0.7959575617927873|    7.0|\n",
      "| 0.4484250584033179|    4.0|\n",
      "| 0.6793959570375868|    6.0|\n",
      "| 0.8017532427858894|    8.0|\n",
      "| 0.6565552949992319|    5.0|\n",
      "| 0.2515595782593636|    2.0|\n",
      "| 0.2073428376111074|    1.0|\n",
      "| 0.6392921379278927|    5.0|\n",
      "| 0.8505582285081454|    9.0|\n",
      "| 0.8184715436384177|    8.0|\n",
      "| 0.7555506990689408|    7.0|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the Bucketizer\n",
    "bucketizer = Bucketizer(splits=quantiles, inputCol=\"values\", outputCol=\"buckets\")\n",
    "\n",
    "# Apply the Bucketizer\n",
    "df_buck = bucketizer.transform(df)\n",
    "\n",
    "df_buck.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a151b8dd-6ea3-4202-ad44-8c51ed2fb664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|buckets|count|\n",
      "+-------+-----+\n",
      "|    8.0|   10|\n",
      "|    0.0|    8|\n",
      "|    7.0|   10|\n",
      "|    4.0|   10|\n",
      "|    2.0|   10|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Frequency table\n",
    "df_result=df_buck.groupBy(\"buckets\").count()\n",
    "\n",
    "# Show the original and bucketed values\n",
    "df_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b139d73-547a-45d3-b2f3-e8f5274ca1c6",
   "metadata": {},
   "source": [
    "## How to stack two DataFrames vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26fd4473-49cd-4ce4-b6fd-9c20b7b3be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n",
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_3|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()\n",
    "\n",
    "# Create DataFrame for region B\n",
    "df_B = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 15), (\"grape\", 4, 6)], [\"Name\", \"Col_1\", \"Col_3\"])\n",
    "df_B.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d75f9632-7aed-4845-971d-746d3fcea185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   15|\n",
      "| grape|    4|    6|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A.union(df_B).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663a120-80e2-4fd3-9b06-772c0954e080",
   "metadata": {},
   "source": [
    "## How to convert the first character of each element in a series to uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "089b8b8e-b059-4a9b-ad5b-150096debd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have the following DataFrame\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda0cfe-0b91-4351-a325-35168dcc242f",
   "metadata": {},
   "source": [
    "### https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.initcap.html?highlight=initcap\n",
    "**pyspark.sql.functions.initcap**\n",
    "\n",
    "Translate the first letter of each word to upper case in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dfe44a6-d640-4697-8af4-bcd8d6cebe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| John|\n",
      "|Alice|\n",
      "|  Bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "\n",
    "# Convert the first character to uppercase\n",
    "df1 = df.withColumn(\"name\", initcap(df[\"name\"]))\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbc0cfbe-bed4-403b-bd00-0b3f1738e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| apple|    3|    5|\n",
      "|banana|    1|   10|\n",
      "|orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for region A\n",
    "df_A = spark.createDataFrame([(\"apple\", 3, 5), (\"banana\", 1, 10), (\"orange\", 2, 8)], [\"Name\", \"Col_1\", \"Col_2\"])\n",
    "df_A.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab513eb5-c5cd-4b80-a433-da92440bde5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "|  Name|Col_1|Col_2|\n",
      "+------+-----+-----+\n",
      "| Apple|    3|    5|\n",
      "|Banana|    1|   10|\n",
      "|Orange|    2|    8|\n",
      "+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_A = df_A.withColumn(\"Name\", initcap(df_A[\"Name\"]))\n",
    "\n",
    "df_A.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee41f401-3f5d-4ca5-abdc-b1e87a444621",
   "metadata": {},
   "source": [
    "## How to calculate the number of characters in each word in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bae2f686-2f74-4ffe-8a69-382820143ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| john|\n",
      "|alice|\n",
      "|  bob|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"john\",), (\"alice\",), (\"bob\",)]\n",
    "df = spark.createDataFrame(data, [\"name\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "900e239a-e31c-4603-bf84-bca71b4cf3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| name|word_length|\n",
      "+-----+-----------+\n",
      "| john|          4|\n",
      "|alice|          5|\n",
      "|  bob|          3|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('word_length', F.length(df.name))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3eddd7-b726-4f4a-bbcf-53f95d5f22fb",
   "metadata": {},
   "source": [
    "## How to compute difference of differences between consecutive numbers of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "713a8621-6c93-489c-a050-3d1eeba25383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+\n",
      "|   name|age|salary|\n",
      "+-------+---+------+\n",
      "|  James| 34| 55000|\n",
      "|Michael| 30| 70000|\n",
      "| Robert| 37| 60000|\n",
      "|  Maria| 29| 80000|\n",
      "|    Jen| 32| 65000|\n",
      "+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For the sake of example, we'll create a sample DataFrame\n",
    "data = [('James', 34, 55000),\n",
    "('Michael', 30, 70000),\n",
    "('Robert', 37, 60000),\n",
    "('Maria', 29, 80000),\n",
    "('Jen', 32, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\" , \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9d50314-a29d-48bb-a9c6-37e94e1e4a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+------+-----------+\n",
      "|   name|age|salary|         id|\n",
      "+-------+---+------+-----------+\n",
      "|  James| 34| 55000| 8589934592|\n",
      "|Michael| 30| 70000|25769803776|\n",
      "| Robert| 37| 60000|34359738368|\n",
      "|  Maria| 29| 80000|51539607552|\n",
      "|    Jen| 32| 65000|60129542144|\n",
      "+-------+---+------+-----------+\n",
      "\n",
      "<pyspark.sql.window.WindowSpec object at 0x000001E81E1E4D30>\n",
      "+-------+---+------+----------+------+\n",
      "|   name|age|salary|prev_value|  diff|\n",
      "+-------+---+------+----------+------+\n",
      "|  James| 34| 55000|      NULL|     0|\n",
      "|Michael| 30| 70000|     55000| 15000|\n",
      "| Robert| 37| 60000|     70000|-10000|\n",
      "|  Maria| 29| 80000|     60000| 20000|\n",
      "|    Jen| 32| 65000|     80000|-15000|\n",
      "+-------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window specification\n",
    "df = df.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df.show()\n",
    "window = Window.orderBy(\"id\")\n",
    "\n",
    "print (window)\n",
    "\n",
    "# Generate the lag of the variable\n",
    "df = df.withColumn(\"prev_value\", F.lag(df.salary).over(window))\n",
    "\n",
    "# Compute the difference with lag\n",
    "df = df.withColumn(\"diff\", F.when(F.isnull(df.salary - df.prev_value), 0)\n",
    ".otherwise(df.salary - df.prev_value)).drop(\"id\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1874e906-3425-4c2e-94b5-4972fc95c735",
   "metadata": {},
   "source": [
    "# Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a105a529-a4a5-49a2-9bdb-e4b3bede95b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|date_str_1| date_str_2|\n",
      "+----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|\n",
      "|2023-12-31|01 Jan 2010|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example data\n",
    "data = [(\"2023-05-18\",\"01 Jan 2010\",), (\"2023-12-31\", \"01 Jan 2010\",)]\n",
    "df = spark.createDataFrame(data, [\"date_str_1\", \"date_str_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc38c6a0-565a-4a33-9824-5805f050f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|date_str_1| date_str_2|    date_1|    date_2|day_of_month|week_number|day_of_year|day_of_week|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "|2023-05-18|01 Jan 2010|2023-05-18|2010-01-01|          18|         20|        138|          5|\n",
      "|2023-12-31|01 Jan 2010|2023-12-31|2010-01-01|          31|         52|        365|          1|\n",
      "+----------+-----------+----------+----------+------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, dayofmonth, weekofyear, dayofyear, dayofweek\n",
    "\n",
    "# Convert date string to date format\n",
    "df = df.withColumn(\"date_1\", to_date(df.date_str_1, 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"date_2\", to_date(df.date_str_2, 'dd MMM yyyy'))\n",
    "\n",
    "df = df.withColumn(\"day_of_month\", dayofmonth(df.date_1))\\\n",
    ".withColumn(\"week_number\", weekofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_year\", dayofyear(df.date_1))\\\n",
    ".withColumn(\"day_of_week\", dayofweek(df.date_1))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf9c9c6-ceb9-4179-a0b0-7610c1ef60b8",
   "metadata": {},
   "source": [
    "# File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59889c-55fd-497c-9936-0bd91206adbe",
   "metadata": {},
   "source": [
    "## How to get the nrows, ncolumns, datatype of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cd19bd7-ffb5-484a-a0bb-bc8dd4aa4b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|RowNumber|CustomerId|Surname |CreditScore|Geography|Gender|Age|Tenure|Balance  |NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "|1        |15634602  |Hargrave|619        |France   |Female|42 |2     |0        |1            |1        |1             |101348.88      |1     |\n",
      "|2        |15647311  |Hill    |608        |Spain    |Female|41 |1     |83807.86 |1            |0        |1             |112542.58      |0     |\n",
      "|3        |15619304  |Onio    |502        |France   |Female|42 |8     |159660.8 |3            |1        |0             |113931.57      |1     |\n",
      "|4        |15701354  |Boni    |699        |France   |Female|39 |1     |0        |2            |0        |0             |93826.63       |0     |\n",
      "|5        |15737888  |Mitchell|850        |Spain    |Female|43 |2     |125510.82|1            |1        |1             |79084.1        |0     |\n",
      "+---------+----------+--------+-----------+---------+------+---+------+---------+-------------+---------+--------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df = spark.read.csv(\"./Churn_Modelling.csv\", header=True, inferSchema=True)\n",
    "churn = spark.read.csv('./Churn_Modelling.csv',header=True,escape=\"\\\"\")\n",
    "churn.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b4b5058-0b67-4b5a-b324-acf510f46f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows:  2\n",
      "Number of Columns:  8\n",
      "Data types:  [('date_str_1', 'string'), ('date_str_2', 'string'), ('date_1', 'date'), ('date_2', 'date'), ('day_of_month', 'int'), ('week_number', 'int'), ('day_of_year', 'int'), ('day_of_week', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# For number of rows\n",
    "nrows = df.count()\n",
    "print(\"Number of Rows: \", nrows)\n",
    "\n",
    "# For number of columns\n",
    "ncols = len(df.columns)\n",
    "print(\"Number of Columns: \", ncols)\n",
    "\n",
    "# For data types of each column\n",
    "datatypes = df.dtypes\n",
    "print(\"Data types: \", datatypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f410f4-208c-4462-972e-c83a70522b67",
   "metadata": {},
   "source": [
    "## How to check if a dataframe has any missing values and count of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b0dd8-a4af-436a-953e-c9a151f0fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(\"A\", 1, None),\n",
    "(\"B\", None, \"123\" ),\n",
    "(\"B\", 3, \"456\"),\n",
    "(\"D\", None, None),\n",
    "], [\"Name\", \"Value\", \"id\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975b90da-b2f0-41a7-83b1-f90789c79774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, isnan, count, when\n",
    "\n",
    "missing = df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns))\n",
    "print(missing)\n",
    "\n",
    "\n",
    "has_missing = any(row.asDict().values() for row in missing.collect())\n",
    "print(has_missing)\n",
    "\n",
    "missing_count = missing.collect()[0].asDict()\n",
    "print(missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29575892-0599-402d-8f03-e4a793b0141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([count(when(col(\"Value\").isNull(), 1)).alias(\"num\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19347c35-6e2b-440b-98ed-75b155bb3d3d",
   "metadata": {},
   "source": [
    "## How to change the order of columns of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda83d22-00e4-4922-9641-41e67dcbf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(\"John\", \"Doe\", 30), (\"Jane\", \"Doe\", 25), (\"Alice\", \"Smith\", 22)]\n",
    "\n",
    "# Create DataFrame from the data\n",
    "df = spark.createDataFrame(data, [\"First_Name\", \"Last_Name\", \"Age\"])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7815598-92b3-4350-861d-938459d6f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = [\"Age\", \"First_Name\", \"Last_Name\"]\n",
    "\n",
    "# Reorder the columns\n",
    "df = df.select(*new_order)\n",
    "\n",
    "# Show the DataFrame with reordered columns\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408b58e-3e6a-49c6-ad9d-9583edaab71f",
   "metadata": {},
   "source": [
    "## How to format or suppress scientific notations in a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8dd06b-1cfd-4665-b53c-f520388cb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame df and the column you want to format is 'your_column'\n",
    "df = spark.createDataFrame([(1, 0.000000123), (2, 0.000023456), (3, 0.000345678)], [\"id\", \"your_column\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f71fe-ebe1-422c-a915-fea27470adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "# Determine the number of decimal places you want\n",
    "decimal_places = 10\n",
    "\n",
    "df = df.withColumn(\"your_column\", format_number(\"your_column\", decimal_places))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cadff-7c09-4d52-a60e-509d42fdc5d5",
   "metadata": {},
   "source": [
    "## How to format all the values in a dataframe as percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfa6c3-cce5-4e20-a0c3-d6a20cc7440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [(0.1, .08), (0.2, .06), (0.33, .02)]\n",
    "df = spark.createDataFrame(data, [\"numbers_1\", \"numbers_2\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980a723-7ce1-4bd8-ade2-beb3a82588fe",
   "metadata": {},
   "source": [
    "### https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.lit.html\n",
    "\n",
    "**pyspark.sql.functions.lit**\n",
    "\n",
    "Creates a Column of literal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e570e6c-86b6-44fb-95e4-78df4e372478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, col, lit\n",
    "\n",
    "columns = [\"numbers_1\", \"numbers_2\"]\n",
    "\n",
    "for col_name in columns:\n",
    "df = df.withColumn(col_name, concat((col(col_name) * 100).cast('decimal(10, 2)'), lit(\"%\")))\n",
    "\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
