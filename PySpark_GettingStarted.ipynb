{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2758f4-d786-4300-913f-09c2e0d235b6",
   "metadata": {},
   "source": [
    " # Check if session is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6011dc68-368e-4e87-90b7-44376e00bfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a9fe5ef-3fb0-43b1-ade8-3d038c85067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Pyspark Tutorial\").config(\"spark.memory.offHeap.enabled\",\"true\").config(\"spark.memory.offHeap.size\",\"10g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36a996-62e3-44f2-908c-e1a6fe68c888",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88f3ce7-290b-4f8c-8af3-3d4a102ffd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./imdb_top_1000.csv',header=True,escape=\"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d9be8-5e4d-4e67-a149-6ec9474f2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multiline json file\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\").json('./jupyter/Employee.json')\n",
    "multiline_df.show(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c2408-079e-470f-be7f-9dadded86530",
   "metadata": {},
   "source": [
    "### DataFrame Action Methods\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| show | Displays the top n rows of DataFrame in a tabular form |\n",
    "| count | Returns the number of rows in the DataFrame |\n",
    "| describe,  summary | Computes basic statistics for numeric and string columns |\n",
    "| first, head | Returns the the first row |\n",
    "| collect | Returns an array that contains all rows in this DataFrame |\n",
    "| take | Returns an array of the first n rows in the DataFrame |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea4c14-7eea-4496-9d62-3d40dd73a4fb",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7672f03-3290-49ee-9d7d-fd9bebda63b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+-----------+-------+--------------------+-----------+--------------------+----------+--------------------+--------------+--------------+-------------+--------------+-----------+-----------+\n",
      "|         Poster_Link|        Series_Title|Released_Year|Certificate|Runtime|               Genre|IMDB_Rating|            Overview|Meta_score|            Director|         Star1|         Star2|        Star3|         Star4|No_of_Votes|      Gross|\n",
      "+--------------------+--------------------+-------------+-----------+-------+--------------------+-----------+--------------------+----------+--------------------+--------------+--------------+-------------+--------------+-----------+-----------+\n",
      "|https://m.media-a...|The Shawshank Red...|         1994|          A|142 min|               Drama|        9.3|Two imprisoned me...|        80|      Frank Darabont|   Tim Robbins|Morgan Freeman|   Bob Gunton|William Sadler|    2343110| 28,341,469|\n",
      "|https://m.media-a...|       The Godfather|         1972|          A|175 min|        Crime, Drama|        9.2|An organized crim...|       100|Francis Ford Coppola| Marlon Brando|     Al Pacino|   James Caan|  Diane Keaton|    1620367|134,966,411|\n",
      "|https://m.media-a...|     The Dark Knight|         2008|         UA|152 min|Action, Crime, Drama|          9|When the menace k...|        84|   Christopher Nolan|Christian Bale|  Heath Ledger|Aaron Eckhart| Michael Caine|    2303232|534,858,444|\n",
      "|https://m.media-a...|The Godfather: Pa...|         1974|          A|202 min|        Crime, Drama|          9|The early life an...|        90|Francis Ford Coppola|     Al Pacino|Robert De Niro|Robert Duvall|  Diane Keaton|    1129952| 57,300,000|\n",
      "|https://m.media-a...|        12 Angry Men|         1957|          U| 96 min|        Crime, Drama|          9|A jury holdout at...|        96|        Sidney Lumet|   Henry Fonda|   Lee J. Cobb|Martin Balsam|  John Fiedler|     689845|  4,360,000|\n",
      "+--------------------+--------------------+-------------+-----------+-------+--------------------+-----------+--------------------+----------+--------------------+--------------+--------------+-------------+--------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5) #Display the content of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d34eab-f971-4d6b-83a0-c07b9c85c0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() #Count the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c857a-d744-40cd-92c9-7409809ccd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary() #Compute base statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd79ff-470d-4bf3-9169-ab01c54f7274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first() #Return first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c72b8-130b-4523-b45f-47ceed3f5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect() #Return an array that contains all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1421791d-615e-4478-a6b0-d3ef0e27391e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Poster_Link='https://m.media-amazon.com/images/M/MV5BMDFkYTc0MGEtZmNhMC00ZDIzLWFmNTEtODM1ZmRlYWMwMWFmXkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_UX67_CR0,0,67,98_AL_.jpg', Series_Title='The Shawshank Redemption', Released_Year='1994', Certificate='A', Runtime='142 min', Genre='Drama', IMDB_Rating='9.3', Overview='Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.', Meta_score='80', Director='Frank Darabont', Star1='Tim Robbins', Star2='Morgan Freeman', Star3='Bob Gunton', Star4='William Sadler', No_of_Votes='2343110', Gross='28,341,469'),\n",
       " Row(Poster_Link='https://m.media-amazon.com/images/M/MV5BM2MyNjYxNmUtYTAwNi00MTYxLWJmNWYtYzZlODY3ZTk3OTFlXkEyXkFqcGdeQXVyNzkwMjQ5NzM@._V1_UY98_CR1,0,67,98_AL_.jpg', Series_Title='The Godfather', Released_Year='1972', Certificate='A', Runtime='175 min', Genre='Crime, Drama', IMDB_Rating='9.2', Overview=\"An organized crime dynasty's aging patriarch transfers control of his clandestine empire to his reluctant son.\", Meta_score='100', Director='Francis Ford Coppola', Star1='Marlon Brando', Star2='Al Pacino', Star3='James Caan', Star4='Diane Keaton', No_of_Votes='1620367', Gross='134,966,411'),\n",
       " Row(Poster_Link='https://m.media-amazon.com/images/M/MV5BMTMxNTMwODM0NF5BMl5BanBnXkFtZTcwODAyMTk2Mw@@._V1_UX67_CR0,0,67,98_AL_.jpg', Series_Title='The Dark Knight', Released_Year='2008', Certificate='UA', Runtime='152 min', Genre='Action, Crime, Drama', IMDB_Rating='9', Overview='When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.', Meta_score='84', Director='Christopher Nolan', Star1='Christian Bale', Star2='Heath Ledger', Star3='Aaron Eckhart', Star4='Michael Caine', No_of_Votes='2303232', Gross='534,858,444'),\n",
       " Row(Poster_Link='https://m.media-amazon.com/images/M/MV5BMWMwMGQzZTItY2JlNC00OWZiLWIyMDctNDk2ZDQ2YjRjMWQ0XkEyXkFqcGdeQXVyNzkwMjQ5NzM@._V1_UY98_CR1,0,67,98_AL_.jpg', Series_Title='The Godfather: Part II', Released_Year='1974', Certificate='A', Runtime='202 min', Genre='Crime, Drama', IMDB_Rating='9', Overview='The early life and career of Vito Corleone in 1920s New York City is portrayed, while his son, Michael, expands and tightens his grip on the family crime syndicate.', Meta_score='90', Director='Francis Ford Coppola', Star1='Al Pacino', Star2='Robert De Niro', Star3='Robert Duvall', Star4='Diane Keaton', No_of_Votes='1129952', Gross='57,300,000'),\n",
       " Row(Poster_Link='https://m.media-amazon.com/images/M/MV5BMWU4N2FjNzYtNTVkNC00NzQ0LTg0MjAtYTJlMjFhNGUxZDFmXkEyXkFqcGdeQXVyNjc1NTYyMjg@._V1_UX67_CR0,0,67,98_AL_.jpg', Series_Title='12 Angry Men', Released_Year='1957', Certificate='U', Runtime='96 min', Genre='Crime, Drama', IMDB_Rating='9', Overview='A jury holdout attempts to prevent a miscarriage of justice by forcing his colleagues to reconsider the evidence.', Meta_score='96', Director='Sidney Lumet', Star1='Henry Fonda', Star2='Lee J. Cobb', Star3='Martin Balsam', Star4='John Fiedler', No_of_Votes='689845', Gross='4,360,000')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5) #Take first n row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ad967-4a09-4fd5-bd36-21278588ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show() #Compute summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83ed4b6e-1be6-4e1a-a1c3-f98d02c5deef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Poster_Link', 'string'),\n",
       " ('Series_Title', 'string'),\n",
       " ('Released_Year', 'string'),\n",
       " ('Certificate', 'string'),\n",
       " ('Runtime', 'string'),\n",
       " ('Genre', 'string'),\n",
       " ('IMDB_Rating', 'string'),\n",
       " ('Overview', 'string'),\n",
       " ('Meta_score', 'string'),\n",
       " ('Director', 'string'),\n",
       " ('Star1', 'string'),\n",
       " ('Star2', 'string'),\n",
       " ('Star3', 'string'),\n",
       " ('Star4', 'string'),\n",
       " ('No_of_Votes', 'string'),\n",
       " ('Gross', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes #Return df column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968a81e9-78ea-4fa6-9386-6a2279215eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Poster_Link',\n",
       " 'Series_Title',\n",
       " 'Released_Year',\n",
       " 'Certificate',\n",
       " 'Runtime',\n",
       " 'Genre',\n",
       " 'IMDB_Rating',\n",
       " 'Overview',\n",
       " 'Meta_score',\n",
       " 'Director',\n",
       " 'Star1',\n",
       " 'Star2',\n",
       " 'Star3',\n",
       " 'Star4',\n",
       " 'No_of_Votes',\n",
       " 'Gross']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns #Return the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb35ea9-32d5-4bd9-a1b3-e6996f0c97db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distinct().count() #Count the number of distinct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7fbed2-e912-497f-a42b-b15feb242cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema() #Print the schema of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5854510-c4ca-4ee9-a76b-ef275a43aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d119a7a-01f0-4e02-ae00-03b07c1c2810",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bae082-e170-4d55-9c7a-b93716e33112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Series_Title: string, IMDB_Rating: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"Series_Title\", \"IMDB_Rating\") #select two column of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5572e4cd-9d88-4ee2-83b7-dc06734f94a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|        Series_Title|IMDB_Rating|\n",
      "+--------------------+-----------+\n",
      "|The Shawshank Red...|        9.3|\n",
      "|       The Godfather|        9.2|\n",
      "|     The Dark Knight|          9|\n",
      "|The Godfather: Pa...|          9|\n",
      "|        12 Angry Men|          9|\n",
      "|The Lord of the R...|        8.9|\n",
      "|        Pulp Fiction|        8.9|\n",
      "|    Schindler's List|        8.9|\n",
      "|           Inception|        8.8|\n",
      "|          Fight Club|        8.8|\n",
      "|The Lord of the R...|        8.8|\n",
      "|        Forrest Gump|        8.8|\n",
      "|Il buono, il brut...|        8.8|\n",
      "|The Lord of the R...|        8.7|\n",
      "|          The Matrix|        8.7|\n",
      "|          Goodfellas|        8.7|\n",
      "|Star Wars: Episod...|        8.7|\n",
      "|One Flew Over the...|        8.7|\n",
      "|            Hamilton|        8.6|\n",
      "|        Gisaengchung|        8.6|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Series_Title\", \"IMDB_Rating\").show() #select two column of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d7213-e661-45a0-b475-35ccfdf7dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df[\"Series_Title\"],df[\"IMDB_Rating\"]).where(int(\"IMDB_Rating\") >  9).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d0c684-152c-45ae-83b0-448466d654f0",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201af89-784d-4dba-a256-df6a37d2e01f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.filter(df[\"IMDB_Rating\"] > 9.0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c5c7b-5e33-44eb-8359-2625293d4f67",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839dd7fd-0882-4cee-b2de-bb5330e515fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "spark = SparkSession.builder.getOrCreate()  \n",
    "  \n",
    "# Read the JSON file  \n",
    "dfj = spark.read.option(\"multiline\",\"true\").json(\"./jupyter/people.json\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fa68d1-1d0c-4e67-83bd-225645d4805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfj.show(100) # Show the data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8982b-f454-42c4-8571-e6d896c1fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfj.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf44d2-40d4-4d3b-9d82-25ededb13475",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfj.select(\"personal_info.email\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df799b07-5246-4b24-86bc-2de1d341bb71",
   "metadata": {},
   "source": [
    "# Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549e39f-d804-4020-bf64-9dd4206c3e1b",
   "metadata": {},
   "source": [
    "### Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11b04d15-8a12-4d40-b95b-36b5cb97edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "  \n",
    "# Initialize SparkSession  \n",
    "spark = SparkSession.builder.appName('Lazy Evaluation Example').getOrCreate()  \n",
    "  \n",
    "# Load data into DataFrame  \n",
    "dfj = spark.read.option(\"multiline\",\"true\").json(\"./people.json\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1991440-dd73-435b-b423-a09e3c92c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----------+\n",
      "| id|       personal_info|profession|\n",
      "+---+--------------------+----------+\n",
      "|  1|{30, john.doe@exa...|  Engineer|\n",
      "|  2|{25, jane.doe@exa...|    Doctor|\n",
      "| 20|{40, person20@exa...|   Teacher|\n",
      "+---+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfj.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c130a3-296a-40f7-b9e0-dbc8efe2294a",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "533457d6-8298-47ac-a0cd-410818c9ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 1: Select 'personal_info' column  \n",
    "df1 = dfj.select('personal_info')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc56eb25-feca-4318-953b-bed1721e3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation 2: Filter records where age is greater than 30  \n",
    "df2 = df1.filter(df1.personal_info.age > 30)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d401ec1-4239-4b42-9f48-ab70baa811f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, perform an action: Count the number of records  \n",
    "count = df2.count()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2775bbc-eaa7-467b-9595-e0ef99b22452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records where age is greater than 30: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# At this point, no actual execution has happened.  \n",
    "# Transformations are just recorded internally by Spark.  \n",
    "  \n",
    "\n",
    "  \n",
    "# This is when Spark actually executes the transformations,  \n",
    "# because it needs the result of 'count()' to proceed.  \n",
    "print(\"Number of records where age is greater than 30:\", count)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425377a-b9bc-4141-a08f-7e19cee5fdae",
   "metadata": {},
   "source": [
    "## Panda DataFrame to PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d3037c00-768e-4609-bc16-47e209b73879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import pandas as pd\n",
    "# Initialize SparkSession  \n",
    "spark = SparkSession.builder.appName('File Example').getOrCreate()\n",
    "\n",
    "euro12 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv', sep=',')\n",
    "\n",
    "euro12df=spark.createDataFrame(euro12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "137235a2-3909-415d-a0d6-429f0aea15fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o385.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 50.0 failed 1 times, most recent failure: Lost task 4.0 in stage 50.0 (TID 68) (LAPTOP-BNE6BCN6.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-d356d9459d36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meuro12df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m-> 1234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o385.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 50.0 failed 1 times, most recent failure: Lost task 4.0 in stage 50.0 (TID 68) (LAPTOP-BNE6BCN6.home executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "euro12df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12303244-8638-4865-bac1-b9b32c2a0448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
